---
title: "Cloud Demo Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Cloud Demo Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      comment  = "#>")
```

This tutorial explains a recommended process for setting up a project that will use the Azure Cloud Batch Service.

## Using a Research Compendium
First I set up the project as a compendium by creating a new GitHub repo on GitHub, cloning it by starting a new project from version control in RStudio, and running `rcompendium::new_compendium(create_repo = FALSE, renv = TRUE)`. A research compendium is just a template folder structure that can be used to organize projects with a goal of making them reproducible. I used `renv = TRUE` so that the project will be set up with an renv lockfile that tracks the packages and versions that are used in the analysis.

I then started writing my first script in "analyses/01_run_model.R". As I use packages I either explicitly show where the function came from by using `packagename::function()` or I can load the whole package by adding #' @import package (or one function with #' @importFrom package function) to the "R/cloudDemo-package.R" file. This ensures that the scripts work the same anytime the project is loaded with `devtools::load_all()` rather than being sensitive to which `library(package)` calls have happened and in which order. I write a function to do a repetitive task and store it in "R/do_mod.R" with some documentation to explain how it works. I store any figures or outputs created in the folders with the same name. Finally,  I check for dependencies and record them in the DESCRIPTION file by running `rcompendium::add_dependencies(".")`. I also update the renv lockfile by running `renv::snapshot()`.

Once the script is ready I add a line to the "make.R" file that will source my script. I also set a global ggplot2 theme in the make file so it will be used in all scripts. To check everything is up-to-date and to run the "make.R" file I first restart R and then run `rcompendium::refresh(".", make = TRUE)`. This will ensure that your script works from a fresh R session. 

## Running the analysis in parallel
So far our analysis is run sequentially so it will not be able to take advantage of the multiple cores available on a cloud machine. To make use of parallel computing we can set up our script to run each model on a separate core. Because this example is so simple I have added a delay in the function so we can see the benefit of running in parallel. There are many ways to do this and I will explain a few options. One option is to create a parallel backend in R using something like the future package. This can be quite straight forward and there are packages to connect this familiar methods of iterating (eg for loops, lapply or purrr). This can get a bit tricky in that you are typically running only parts of the script on multiple cores you need to make sure the right dependencies are available on the workers. future manages this for the most part but see https://future.futureverse.org/articles/future-4-issues.html for tips when that fails.  

### Using the future and furrr packages

The future package sets up the infrastructure needed to run things in the background. To initialize this you run `future::plan("multisession")` which will use all `future::availableCores()` by default. See script "analyses/02_run_model_furrr.R" for an example. 

### Using future and foreach

If you usually use for loops then the foreach package might be easier to use. This can be used with future to create the backend using the doFuture package. 


